{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-07 18:48:11.216525: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# pip install transformers pandas scikit-learn pytorch tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>chords</th>\n",
       "      <th>release_date</th>\n",
       "      <th>genres</th>\n",
       "      <th>decade</th>\n",
       "      <th>rock_genre</th>\n",
       "      <th>artist_id</th>\n",
       "      <th>main_genre</th>\n",
       "      <th>spotify_song_id</th>\n",
       "      <th>spotify_artist_id</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>song_title</th>\n",
       "      <th>estimated_key</th>\n",
       "      <th>roman_chords</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;intro_1&gt; E D A/Cs E D A/Cs &lt;verse_1&gt; E D A/Cs...</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>'alternative metal' 'alternative rock' 'nu met...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>pop rock</td>\n",
       "      <td>artist_2</td>\n",
       "      <td>metal</td>\n",
       "      <td>2ffJZ2r8HxI5DHcmf3BO6c</td>\n",
       "      <td>694QW15WkebjcrWgQHzRYF</td>\n",
       "      <td>Everclear</td>\n",
       "      <td>I Want To Die A Beautiful Death</td>\n",
       "      <td>C</td>\n",
       "      <td>&lt;intro_1&gt; III II VI/Is III II VI/Is &lt;verse_1&gt; ...</td>\n",
       "      <td>Everybody here in this small town\\nLooks used ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;intro_1&gt; Csmin &lt;verse_1&gt; A Csmin A Csmin A Cs...</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>'alternative metal' 'canadian rock' 'funk meta...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>canadian rock</td>\n",
       "      <td>artist_3</td>\n",
       "      <td>metal</td>\n",
       "      <td>5KiY8SZEnvCPyIEkFGRR3y</td>\n",
       "      <td>0niJkG4tKkne3zwr7I8n9n</td>\n",
       "      <td>Finger Eleven</td>\n",
       "      <td>Sad Exchange</td>\n",
       "      <td>C</td>\n",
       "      <td>&lt;intro_1&gt; Ismin &lt;verse_1&gt; VI Ismin VI Ismin VI...</td>\n",
       "      <td>Quietly thinking to myself\\nSharing half our m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;intro_1&gt; D Dmaj7 D Dmaj7 &lt;verse_1&gt; Emin A D G...</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01TtAcUqyLCRBZq4ZZiQWS</td>\n",
       "      <td>17BfKBemmMGO5ZAK25wraW</td>\n",
       "      <td>RAPHA</td>\n",
       "      <td>Funny</td>\n",
       "      <td>D</td>\n",
       "      <td>&lt;intro_1&gt; I Imaj7 I Imaj7 &lt;verse_1&gt; IImin V I ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;intro_1&gt; C &lt;verse_1&gt; G C G C &lt;chorus_1&gt; F Dmi...</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>'modern country pop'</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist_5</td>\n",
       "      <td>pop</td>\n",
       "      <td>3zUecdrWC3IqrNSjhnoF3G</td>\n",
       "      <td>4GGfAshSkqoxpZdoaHm7ky</td>\n",
       "      <td>ERNEST</td>\n",
       "      <td>Anything But Sober</td>\n",
       "      <td>Gm</td>\n",
       "      <td>&lt;intro_1&gt; iv &lt;verse_1&gt; i iv i iv &lt;chorus_1&gt; bV...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;intro_1&gt; C G C G &lt;verse_1&gt; C G C G C Bmin Emi...</td>\n",
       "      <td>2018-08-24</td>\n",
       "      <td>'classic opm' 'opm'</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>artist_6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2zGr6pFE273K7nnu8ACKjk</td>\n",
       "      <td>33kC1w8s5cXEi1zyJZuqSI</td>\n",
       "      <td>Paolo Santos</td>\n",
       "      <td>Come On Home</td>\n",
       "      <td>G</td>\n",
       "      <td>&lt;intro_1&gt; IV I IV I &lt;verse_1&gt; IV I IV I IV III...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id                                             chords  \\\n",
       "0           1   2  <intro_1> E D A/Cs E D A/Cs <verse_1> E D A/Cs...   \n",
       "1           2   3  <intro_1> Csmin <verse_1> A Csmin A Csmin A Cs...   \n",
       "2           3   4  <intro_1> D Dmaj7 D Dmaj7 <verse_1> Emin A D G...   \n",
       "3           4   5  <intro_1> C <verse_1> G C G C <chorus_1> F Dmi...   \n",
       "4           5   6  <intro_1> C G C G <verse_1> C G C G C Bmin Emi...   \n",
       "\n",
       "  release_date                                             genres  decade  \\\n",
       "0   2003-01-01  'alternative metal' 'alternative rock' 'nu met...  2000.0   \n",
       "1   2003-01-01  'alternative metal' 'canadian rock' 'funk meta...  2000.0   \n",
       "2   2022-09-23                                                NaN  2020.0   \n",
       "3   2023-02-10                               'modern country pop'  2020.0   \n",
       "4   2018-08-24                                'classic opm' 'opm'  2010.0   \n",
       "\n",
       "      rock_genre artist_id main_genre         spotify_song_id  \\\n",
       "0       pop rock  artist_2      metal  2ffJZ2r8HxI5DHcmf3BO6c   \n",
       "1  canadian rock  artist_3      metal  5KiY8SZEnvCPyIEkFGRR3y   \n",
       "2            NaN  artist_4        NaN  01TtAcUqyLCRBZq4ZZiQWS   \n",
       "3            NaN  artist_5        pop  3zUecdrWC3IqrNSjhnoF3G   \n",
       "4            NaN  artist_6        NaN  2zGr6pFE273K7nnu8ACKjk   \n",
       "\n",
       "        spotify_artist_id    artist_name                       song_title  \\\n",
       "0  694QW15WkebjcrWgQHzRYF      Everclear  I Want To Die A Beautiful Death   \n",
       "1  0niJkG4tKkne3zwr7I8n9n  Finger Eleven                     Sad Exchange   \n",
       "2  17BfKBemmMGO5ZAK25wraW          RAPHA                            Funny   \n",
       "3  4GGfAshSkqoxpZdoaHm7ky         ERNEST               Anything But Sober   \n",
       "4  33kC1w8s5cXEi1zyJZuqSI   Paolo Santos                     Come On Home   \n",
       "\n",
       "  estimated_key                                       roman_chords  \\\n",
       "0             C  <intro_1> III II VI/Is III II VI/Is <verse_1> ...   \n",
       "1             C  <intro_1> Ismin <verse_1> VI Ismin VI Ismin VI...   \n",
       "2             D  <intro_1> I Imaj7 I Imaj7 <verse_1> IImin V I ...   \n",
       "3            Gm  <intro_1> iv <verse_1> i iv i iv <chorus_1> bV...   \n",
       "4             G  <intro_1> IV I IV I <verse_1> IV I IV I IV III...   \n",
       "\n",
       "                                              lyrics  \n",
       "0  Everybody here in this small town\\nLooks used ...  \n",
       "1  Quietly thinking to myself\\nSharing half our m...  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 440134 entries, 0 to 440133\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   Unnamed: 0         440134 non-null  int64  \n",
      " 1   id                 440134 non-null  int64  \n",
      " 2   chords             440134 non-null  object \n",
      " 3   release_date       422034 non-null  object \n",
      " 4   genres             376364 non-null  object \n",
      " 5   decade             422034 non-null  float64\n",
      " 6   rock_genre         130596 non-null  object \n",
      " 7   artist_id          440134 non-null  object \n",
      " 8   main_genre         313653 non-null  object \n",
      " 9   spotify_song_id    440134 non-null  object \n",
      " 10  spotify_artist_id  440134 non-null  object \n",
      " 11  artist_name        440134 non-null  object \n",
      " 12  song_title         440134 non-null  object \n",
      " 13  estimated_key      440134 non-null  object \n",
      " 14  roman_chords       440134 non-null  object \n",
      " 15  lyrics             284637 non-null  object \n",
      "dtypes: float64(1), int64(2), object(13)\n",
      "memory usage: 53.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                0\n",
       "id                        0\n",
       "chords                    0\n",
       "release_date          18100\n",
       "genres                63770\n",
       "decade                18100\n",
       "rock_genre           309538\n",
       "artist_id                 0\n",
       "main_genre           126481\n",
       "spotify_song_id           0\n",
       "spotify_artist_id         0\n",
       "artist_name               0\n",
       "song_title                0\n",
       "estimated_key             0\n",
       "roman_chords              0\n",
       "lyrics               155497\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['lyrics'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chords_by_section(chord_sequence):\n",
    "    \"\"\"\n",
    "    Extract chords divided by section from a chord sequence string.\n",
    "    \n",
    "    Args:\n",
    "        chord_sequence (str): The full chord progression string with section markers\n",
    "            Example: \"<intro_1> C <verse_1> F C E7 Amin C F C G7 C F C G7\"\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are section names and values are lists of chords\n",
    "            Example: {\n",
    "                \"intro_1\": [\"C\"],\n",
    "                \"verse_1\": [\"F\", \"C\", \"E7\", \"Amin\", \"C\", \"F\", \"C\", \"G7\", \"C\", \"F\", \"C\", \"G7\"]\n",
    "            }\n",
    "    \"\"\"\n",
    "    if not isinstance(chord_sequence, str) or not chord_sequence.strip():\n",
    "        return {}\n",
    "    \n",
    "    sections = {}\n",
    "    current_section = None\n",
    "    current_chords = []\n",
    "    \n",
    "    # Split the chord sequence into tokens\n",
    "    tokens = chord_sequence.split()\n",
    "    \n",
    "    # Regular expression to identify section markers like <verse_1>\n",
    "    import re\n",
    "    section_pattern = re.compile(r'<([^>]+)>')\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Check if this token is a section marker\n",
    "        section_match = section_pattern.match(token)\n",
    "        \n",
    "        if section_match:\n",
    "            # Save the previous section if it exists\n",
    "            if current_section and current_chords:\n",
    "                sections[current_section] = current_chords\n",
    "            \n",
    "            # Start a new section\n",
    "            current_section = section_match.group(1)\n",
    "            current_chords = []\n",
    "        else:\n",
    "            # Add this chord to the current section\n",
    "            current_chords.append(token)\n",
    "    \n",
    "    # Add the last section\n",
    "    if current_section and current_chords:\n",
    "        sections[current_section] = current_chords\n",
    "    \n",
    "    return sections\n",
    "\n",
    "# Example usage:\n",
    "# If you have a pandas DataFrame with a 'chords' column\n",
    "df['chords_by_section'] = df['chords'].apply(extract_chords_by_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         {'intro_1': ['E', 'D', 'A/Cs', 'E', 'D', 'A/Cs...\n",
       "1         {'intro_1': ['Csmin'], 'verse_1': ['A', 'Csmin...\n",
       "8         {'chorus_1': ['Amin', 'G', 'F', 'G', 'Amin', '...\n",
       "11        {'intro_1': ['D', 'G', 'D', 'G', 'D', 'G', 'D'...\n",
       "12        {'intro_1': ['E', 'G', 'D', 'A'], 'verse_1': [...\n",
       "                                ...                        \n",
       "440096    {'intro_1': ['D', 'G', 'D', 'G', 'D', 'G', 'D'...\n",
       "440100    {'intro_1': ['C', 'C7', 'C', 'C7'], 'verse_1':...\n",
       "440103    {'intro_1': ['Eno3d', 'Ano3d', 'Eno3d', 'B', '...\n",
       "440129    {'intro_1': ['Eb/G', 'Abmaj7', 'Bbadd9', 'Eb',...\n",
       "440130    {'verse_1': ['Bb', 'C', 'Bb', 'Amin', 'Bb', 'A...\n",
       "Name: chords_by_section, Length: 284637, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['chords_by_section']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sharp_notation(chord_sequence):\n",
    "    \"\"\"\n",
    "    Normalize chord notation by converting all 's' sharp indicators to '#'.\n",
    "    \n",
    "    For example:\n",
    "    - 'Cs' becomes 'C#'\n",
    "    - 'Fs' becomes 'F#'\n",
    "    - 'Gs' becomes 'G#'\n",
    "    \n",
    "    Args:\n",
    "        chord_sequence (str or dict): Either a full chord sequence string or a dictionary \n",
    "                                      of sections with chord lists (from extract_chords_by_section)\n",
    "    \n",
    "    Returns:\n",
    "        Same type as input: The normalized chord sequence or dictionary with normalized chords\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Pattern to match a note with 's' sharp indicator\n",
    "    # Looks for any of the notes A-G followed by 's', optionally followed by additional characters\n",
    "    sharp_pattern = re.compile(r'([A-G])s([a-zA-Z0-9/]*)')\n",
    "    \n",
    "    if isinstance(chord_sequence, str):\n",
    "        # For string input, replace directly in the string\n",
    "        normalized = re.sub(sharp_pattern, r'\\1#\\2', chord_sequence)\n",
    "        return normalized\n",
    "    \n",
    "    elif isinstance(chord_sequence, dict):\n",
    "        # For dictionary input (from extract_chords_by_section)\n",
    "        normalized_dict = {}\n",
    "        \n",
    "        for section, chords in chord_sequence.items():\n",
    "            normalized_chords = []\n",
    "            \n",
    "            for chord in chords:\n",
    "                # Apply the replacement to each chord\n",
    "                normalized_chord = re.sub(sharp_pattern, r'\\1#\\2', chord)\n",
    "                normalized_chords.append(normalized_chord)\n",
    "            \n",
    "            normalized_dict[section] = normalized_chords\n",
    "        \n",
    "        return normalized_dict\n",
    "    \n",
    "    else:\n",
    "        # Return unchanged if input is neither string nor dict\n",
    "        return chord_sequence\n",
    "\n",
    "\n",
    "# Example usage with a string:\n",
    "# chord_str = \"<intro_1> Cs <verse_1> F Cs Gs7 Amin Cs F Cs As7\"\n",
    "# normalized_str = normalize_sharp_notation(chord_str)\n",
    "# print(normalized_str)\n",
    "# Output: \"<intro_1> C# <verse_1> F C# G#7 Amin C# F C# A#7\"\n",
    "\n",
    "# Example usage with a dictionary from extract_chords_by_section:\n",
    "# sections_dict = extract_chords_by_section(chord_str)\n",
    "# normalized_dict = normalize_sharp_notation(sections_dict)\n",
    "# print(normalized_dict)\n",
    "# Output: {'intro_1': ['C#'], 'verse_1': ['F', 'C#', 'G#7', 'Amin', 'C#', 'F', 'C#', 'A#7']}\n",
    "\n",
    "df['chords_by_section'] = df['chords_by_section'].apply(normalize_sharp_notation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         {'intro_1': ['III', 'II', 'VI/Is', 'III', 'II'...\n",
       "1         {'intro_1': ['Ismin'], 'verse_1': ['VI', 'Ismi...\n",
       "8         {'chorus_1': ['imin', 'bVII', 'bVI', 'bVII', '...\n",
       "11        {'intro_1': ['III', 'VI', 'III', 'VI', 'III', ...\n",
       "12        {'intro_1': ['V', 'bVII', 'IV', 'I'], 'verse_1...\n",
       "                                ...                        \n",
       "440096    {'intro_1': ['V', 'I', 'V', 'I', 'V', 'I', 'V'...\n",
       "440100    {'intro_1': ['I', 'I7', 'I', 'I7'], 'verse_1':...\n",
       "440103    {'intro_1': ['Ino3d', 'IVno3d', 'Ino3d', 'V', ...\n",
       "440129    {'intro_1': ['bVII/ii', 'bIIImaj7', 'ivadd9', ...\n",
       "440130    {'verse_1': ['bII', 'bIII', 'bII', 'imin', 'bI...\n",
       "Name: roman_chords_by_section, Length: 284637, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['roman_chords_by_section'] = df['roman_chords'].apply(extract_chords_by_section)\n",
    "df['roman_chords_by_section'] = df['roman_chords_by_section'].apply(normalize_sharp_notation)\n",
    "df['roman_chords_by_section']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChordProgressionGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6, feature_dim=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text prompt encoder (uses pre-trained model)\n",
    "        self.text_encoder = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.feature_dim = feature_dim\n",
    "        # Freeze text encoder to preserve pre-trained knowledge\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Additional features encoder (genre, decade, etc.)\n",
    "        self.feature_encoder = nn.Linear(feature_dim, d_model)\n",
    "        \n",
    "        # Decoder for chord progression generation\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Output projection to chord vocabulary\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, prompt_ids, prompt_mask, features, tgt_chords, tgt_mask):\n",
    "        # Encode text prompt\n",
    "        prompt_encoding = self.text_encoder(prompt_ids, attention_mask=prompt_mask).last_hidden_state\n",
    "        \n",
    "        # Encode additional features\n",
    "        feature_encoding = self.feature_encoder(features)\n",
    "        \n",
    "        # Combine encodings\n",
    "        encoder_output = torch.cat([prompt_encoding, feature_encoding], dim=1)\n",
    "        \n",
    "        # Generate chord progression\n",
    "        decoder_output = self.decoder(tgt_chords, encoder_output, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(decoder_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chord tokenizer to convert chords to token IDs\n",
    "class ChordTokenizer:\n",
    "    def __init__(self):\n",
    "        self.chord_to_id = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.id_to_chord = {v: k for k, v in self.chord_to_id.items()}\n",
    "        self.section_pattern = re.compile(r'<([^>]+)>')\n",
    "        \n",
    "    def fit(self, chord_sequences):\n",
    "        \"\"\"Build vocabulary from chord sequences.\"\"\"\n",
    "        all_chords = set()\n",
    "        for seq in chord_sequences:\n",
    "            if isinstance(seq, str):\n",
    "                # Split by space and filter out section markers\n",
    "                chords = [c for c in seq.split() if not self.section_pattern.match(c)]\n",
    "                all_chords.update(chords)\n",
    "        \n",
    "        # Add all unique chords to vocabulary\n",
    "        for i, chord in enumerate(sorted(all_chords)):\n",
    "            idx = len(self.chord_to_id)\n",
    "            self.chord_to_id[chord] = idx\n",
    "            self.id_to_chord[idx] = chord\n",
    "            \n",
    "        print(f\"Created chord vocabulary with {len(self.chord_to_id)} tokens\")\n",
    "        return self\n",
    "    \n",
    "    def tokenize(self, chord_sequence, max_length=50, add_special_tokens=True):\n",
    "        \"\"\"Convert chord sequence to token IDs.\"\"\"\n",
    "        if not isinstance(chord_sequence, str) or not chord_sequence.strip():\n",
    "            return [1, 2] + [0] * (max_length - 2) if add_special_tokens else [0] * max_length\n",
    "        \n",
    "        # Initialize with BOS token if needed\n",
    "        tokens = [1] if add_special_tokens else []\n",
    "        \n",
    "        # Process each chord\n",
    "        chords = chord_sequence.split()\n",
    "        for chord in chords:\n",
    "            # Check if it's a section marker\n",
    "            if self.section_pattern.match(chord):\n",
    "                continue  # Skip section markers for now\n",
    "            \n",
    "            # Add chord token\n",
    "            token_id = self.chord_to_id.get(chord, self.chord_to_id[\"<UNK>\"])\n",
    "            tokens.append(token_id)\n",
    "        \n",
    "        # Add EOS token if needed\n",
    "        if add_special_tokens:\n",
    "            tokens.append(2)\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(tokens) < max_length:\n",
    "            tokens += [0] * (max_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:max_length-1] + ([2] if add_special_tokens else [])\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert token IDs back to chord sequence.\"\"\"\n",
    "        chords = []\n",
    "        for token_id in token_ids:\n",
    "            # Skip special tokens\n",
    "            if token_id in [0, 1, 2]:\n",
    "                continue\n",
    "            \n",
    "            chord = self.id_to_chord.get(token_id, \"<UNK>\")\n",
    "            chords.append(chord)\n",
    "        \n",
    "        return \" \".join(chords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the PyTorch dataset for lyrics-to-chord generation\n",
    "class LyricsChordDataset(Dataset):\n",
    "    def __init__(self, df, text_tokenizer, chord_tokenizer, max_lyrics_length=128, max_chord_length=50):\n",
    "        self.df = df\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.chord_tokenizer = chord_tokenizer\n",
    "        self.max_lyrics_length = max_lyrics_length\n",
    "        self.max_chord_length = max_chord_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Get lyrics\n",
    "        lyrics = row['lyrics'] if not pd.isna(row['lyrics']) else \"\"\n",
    "            \n",
    "        # Tokenize the lyrics\n",
    "        lyrics_encoding = self.text_tokenizer(\n",
    "            lyrics,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_lyrics_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get chord tokens - using the original chords column for simplicity \n",
    "        # (could also use normalized_chords if you created that column)\n",
    "        chord_sequence = row['roman_chords']\n",
    "        chord_tokens = torch.tensor(\n",
    "            self.chord_tokenizer.tokenize(chord_sequence, self.max_chord_length)\n",
    "        )\n",
    "        \n",
    "        # No genre features, just lyrics-to-chord mapping\n",
    "        return {\n",
    "            \"prompt_ids\": lyrics_encoding[\"input_ids\"].flatten(),\n",
    "            \"prompt_attention_mask\": lyrics_encoding[\"attention_mask\"].flatten(),\n",
    "            \"chord_tokens\": chord_tokens,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional encoding for transformer models\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated model architecture for lyrics-to-chord generation\n",
    "class LyricsToChordModel(nn.Module):\n",
    "    def __init__(self, chord_vocab_size, text_model_name=\"distilbert-base-uncased\", \n",
    "                 d_model=256, nhead=8, num_encoder_layers=6, num_decoder_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Text encoder (frozen pre-trained model)\n",
    "        self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "        \n",
    "        # Freeze the text encoder weights\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Text projection\n",
    "        self.text_projection = nn.Linear(self.text_encoder.config.hidden_size, d_model)\n",
    "        \n",
    "        # Chord embedding\n",
    "        self.chord_embedding = nn.Embedding(chord_vocab_size, d_model, padding_idx=0)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=150)\n",
    "        \n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, chord_vocab_size)\n",
    "        \n",
    "        # Save hyperparameters\n",
    "        self.d_model = d_model\n",
    "        self.chord_vocab_size = chord_vocab_size\n",
    "\n",
    "    def encode(self, prompt_ids, prompt_attention_mask):\n",
    "        \"\"\"Encode the input lyrics.\"\"\"\n",
    "        # Process text prompt\n",
    "        text_outputs = self.text_encoder(\n",
    "            input_ids=prompt_ids,\n",
    "            attention_mask=prompt_attention_mask\n",
    "        )\n",
    "        text_hidden = text_outputs.last_hidden_state\n",
    "        \n",
    "        # Project text hidden states\n",
    "        src = self.text_projection(text_hidden)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # Create source mask for padding tokens\n",
    "        src_key_padding_mask = prompt_attention_mask.eq(0)\n",
    "        \n",
    "        # Run through encoder\n",
    "        memory = self.transformer.encoder(\n",
    "            src.transpose(0, 1),\n",
    "            src_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        return memory, src_key_padding_mask\n",
    "    \n",
    "    def decode(self, tgt, memory, src_key_padding_mask):\n",
    "        \"\"\"Decode the target sequence using the encoder memory.\"\"\"\n",
    "        # Embed target tokens\n",
    "        tgt_emb = self.chord_embedding(tgt)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        \n",
    "        # Create target mask (causal/autoregressive)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_mask = torch.triu(\n",
    "            torch.ones(tgt_len, tgt_len, device=tgt.device) * float('-inf'), \n",
    "            diagonal=1\n",
    "        )\n",
    "        \n",
    "        # Create target padding mask\n",
    "        tgt_key_padding_mask = tgt.eq(0)\n",
    "        \n",
    "        # Run through decoder\n",
    "        decoder_output = self.transformer.decoder(\n",
    "            tgt_emb.transpose(0, 1),\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=src_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(decoder_output.transpose(0, 1))\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def forward(self, prompt_ids, prompt_attention_mask, chord_tokens):\n",
    "        \"\"\"Full forward pass.\"\"\"\n",
    "        # Encode prompt\n",
    "        memory, src_key_padding_mask = self.encode(prompt_ids, prompt_attention_mask)\n",
    "        \n",
    "        # Use teacher forcing for training\n",
    "        # Shift right to create decoder input (remove last token, add BOS at beginning)\n",
    "        tgt_input = chord_tokens[:, :-1]\n",
    "        \n",
    "        # Decode\n",
    "        logits = self.decode(tgt_input, memory, src_key_padding_mask)\n",
    "        \n",
    "        # Prepare target (shift left: remove first token which is BOS)\n",
    "        target = chord_tokens[:, 1:]\n",
    "        \n",
    "        return logits, target\n",
    "    \n",
    "    def generate(self, prompt_ids, prompt_attention_mask, max_length=50, \n",
    "                 temperature=1.0, top_k=0, top_p=0.9):\n",
    "        \"\"\"Generate a chord progression from lyrics.\"\"\"\n",
    "        batch_size = prompt_ids.size(0)\n",
    "        device = prompt_ids.device\n",
    "        \n",
    "        # Encode prompt\n",
    "        memory, src_key_padding_mask = self.encode(prompt_ids, prompt_attention_mask)\n",
    "        \n",
    "        # Start with BOS token\n",
    "        current_token = torch.full((batch_size, 1), 1, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Storage for generated tokens\n",
    "        generated = [current_token]\n",
    "        \n",
    "        # Generation loop\n",
    "        for _ in range(max_length - 1):\n",
    "            # Decode current sequence\n",
    "            logits = self.decode(current_token, memory, src_key_padding_mask)\n",
    "            \n",
    "            # Get the next token probabilities\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "                next_token_logits = torch.zeros_like(next_token_logits).scatter_(-1, top_k_indices, top_k_logits)\n",
    "            \n",
    "            # Apply top-p (nucleus) filtering\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                \n",
    "                # Remove tokens with cumulative probability above the threshold\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                # Shift the indices to the right to keep the first token above threshold\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                \n",
    "                # Create a mask for indices to remove\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                    -1, sorted_indices, sorted_indices_to_remove\n",
    "                )\n",
    "                next_token_logits[indices_to_remove] = -float('inf')\n",
    "            \n",
    "            # Sample from the filtered distribution\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Add to generated\n",
    "            generated.append(next_token)\n",
    "            current_token = torch.cat(generated, dim=1)\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            if (next_token == 2).any():\n",
    "                break\n",
    "        \n",
    "        # Concatenate all tokens\n",
    "        result = torch.cat(generated, dim=1)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training implementation\n",
    "def train_model(df, batch_size=32, epochs=10, learning_rate=1e-4):\n",
    "    \"\"\"Train the lyrics-to-chord progression model.\"\"\"\n",
    "    # Split dataset\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    print(f\"Training with {len(train_df)} examples, validating with {len(val_df)} examples\")\n",
    "    \n",
    "    # Initialize tokenizers\n",
    "    text_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    chord_tokenizer = ChordTokenizer()\n",
    "    \n",
    "    # Build chord vocabulary from training data\n",
    "    chord_tokenizer.fit(train_df['roman_chords'].dropna())\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = LyricsChordDataset(train_df, text_tokenizer, chord_tokenizer)\n",
    "    val_dataset = LyricsChordDataset(val_df, text_tokenizer, chord_tokenizer)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LyricsToChordModel(chord_vocab_size=len(chord_tokenizer.chord_to_id))\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\"):\n",
    "            # Move batch to device\n",
    "            prompt_ids = batch[\"prompt_ids\"].to(device)\n",
    "            prompt_attention_mask = batch[\"prompt_attention_mask\"].to(device)\n",
    "            chord_tokens = batch[\"chord_tokens\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, target = model(prompt_ids, prompt_attention_mask, chord_tokens)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, model.chord_vocab_size),\n",
    "                target.reshape(-1),\n",
    "                ignore_index=0  # Ignore padding\n",
    "            )\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\"):\n",
    "                # Move batch to device\n",
    "                prompt_ids = batch[\"prompt_ids\"].to(device)\n",
    "                prompt_attention_mask = batch[\"prompt_attention_mask\"].to(device)\n",
    "                chord_tokens = batch[\"chord_tokens\"].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                logits, target = model(prompt_ids, prompt_attention_mask, chord_tokens)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = F.cross_entropy(\n",
    "                    logits.reshape(-1, model.chord_vocab_size),\n",
    "                    target.reshape(-1),\n",
    "                    ignore_index=0  # Ignore padding\n",
    "                )\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Log metrics\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"chord_progression_model_2.pt\")\n",
    "            print(f\"Saved new best model with val loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return model, chord_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function\n",
    "def generate_chord_progression_from_lyrics(lyrics, model, text_tokenizer, chord_tokenizer, max_length=10):\n",
    "    \"\"\"Generate a chord progression from lyrics.\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize lyrics\n",
    "    lyrics_encoding = text_tokenizer(\n",
    "        lyrics,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    prompt_ids = lyrics_encoding[\"input_ids\"].to(device)\n",
    "    prompt_attention_mask = lyrics_encoding[\"attention_mask\"].to(device)\n",
    "    \n",
    "    # Generate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(\n",
    "            prompt_ids, \n",
    "            prompt_attention_mask,\n",
    "            max_length=max_length\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    chord_progression = chord_tokenizer.decode(generated[0].cpu().numpy())\n",
    "    \n",
    "    return chord_progression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, chord_tokenizer = train_model(df, batch_size=32, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created chord vocabulary with 5638 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mario/.pyenv/versions/lewagon/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/mario/.pyenv/versions/lewagon/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def quick_model_load(df=df, model_path='chord_progression_model.pt', random_state=42):\n",
    "    \n",
    "    \"\"\"Loads a model that has already been trained, just be careful to use the same random_state value as the one in the test training split.\"\"\"\n",
    "    \n",
    "    chord_tokenizer = ChordTokenizer()\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=random_state)\n",
    "\n",
    "    chord_tokenizer.fit(train_df['roman_chords'].dropna())\n",
    "    model = LyricsToChordModel(chord_vocab_size=len(chord_tokenizer.chord_to_id))\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path,weights_only=True))\n",
    "    return model, chord_tokenizer\n",
    "\n",
    "\n",
    "model, chord_tokenizer = quick_model_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lyrics: In the darkness of the night, I feel alone and lost\n",
      "Generated chord progression: imin bVII bIII v imin v bIII bVII bIII\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a sample lyric\n",
    "sample_lyric = \"In the darkness of the night, I feel alone and lost\"\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Generate chord progression\n",
    "progression = generate_chord_progression_from_lyrics(\n",
    "    sample_lyric, model, text_tokenizer, chord_tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Lyrics: {sample_lyric}\")\n",
    "print(f\"Generated chord progression: {progression}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
